#!/usr/bin/env python3
"""
å¹¶è¡Œå¯è¾¾æ€§éªŒè¯è„šæœ¬
ä½¿ç”¨ multiprocessing å¹¶è¡Œå¤„ç†å¤šæ¡è½¨è¿¹
ä¿æŒå®Œå…¨ç›¸åŒçš„è®¡ç®—ç²¾åº¦
"""

import sys
try:
    import distutils.version
except AttributeError:
    import distutils
    from packaging import version as packaging_version
    distutils.version = type('version', (), {
        'LooseVersion': packaging_version.Version,
        'StrictVersion': packaging_version.Version
    })
from pathlib import Path
import numpy as np
import torch
import pickle
import json
import time
from multiprocessing import Pool, cpu_count
from functools import partial

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from TD3.TD3 import TD3
from verification.polar_verifier import verify_safety


def load_trajectories(pkl_path=None):
    """åŠ è½½ä¿å­˜çš„è½¨è¿¹"""
    if pkl_path is None:
        pkl_path = Path(__file__).parent.parent / "assets" / "trajectories.pkl"
    
    if not pkl_path.exists():
        raise FileNotFoundError(
            f"è½¨è¿¹æ–‡ä»¶ä¸å­˜åœ¨: {pkl_path}\n"
            f"è¯·å…ˆè¿è¡Œ collect_trajectories.py"
        )
    
    with open(pkl_path, 'rb') as f:
        trajectories = pickle.load(f)
    
    valid_trajectories = [t for t in trajectories if t is not None]
    return valid_trajectories


def verify_single_trajectory_worker(args):
    """
    å•ä¸ªè½¨è¿¹çš„éªŒè¯å‡½æ•°ï¼ˆæ·»åŠ ä½å§¿æ”¯æŒï¼‰
    """
    trajectory_idx, trajectory_data, model_path, observation_error, sample_interval = args
    
    # åŠ è½½æ¨¡å‹...
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    agent = TD3(
        state_dim=25,
        action_dim=2,
        max_action=1.0,
        device=device,
        load_model=True,
        model_name="TD3",
        load_directory=model_path,
    )
    
    # ===== æå–çŠ¶æ€å’Œä½å§¿å¹¶é‡‡æ · =====
    states = trajectory_data['states']
    poses = trajectory_data['poses']  # â† æ–°å¢
    
    sampled_states = states[::sample_interval]
    sampled_poses = poses[::sample_interval]  # â† æ–°å¢
    n_samples = len(sampled_states)
    
    print(f"[è¿›ç¨‹ {trajectory_idx+1}] å¼€å§‹éªŒè¯ {n_samples} ä¸ªé‡‡æ ·ç‚¹...")
    
    results = []
    safe_count = 0
    start_time = time.time()
    
    # ===== é€ç‚¹éªŒè¯ï¼ˆæ·»åŠ ä½å§¿ï¼‰ =====
    for i, (state, pose) in enumerate(zip(sampled_states, sampled_poses)):
        step_idx = i * sample_interval
        
        if i % max(1, n_samples // 4) == 0:
            elapsed = time.time() - start_time
            print(f"[è¿›ç¨‹ {trajectory_idx+1}] è¿›åº¦: {i+1}/{n_samples} "
                  f"({i/n_samples*100:.0f}%) | å·²ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ")
        
        # â† ä¿®æ”¹ï¼šä¼ é€’ä½å§¿
        is_safe, action_ranges = verify_safety(
            agent,
            state,
            tuple(pose),  # â† æ–°å¢ï¼š(x, y, Î¸)
            observation_error=observation_error,
            bern_order=1,
            error_steps=4000,
        )
        
        # åç»­å¤„ç†ä¸å˜...
        det_action = agent.get_action(state, add_noise=False)
        width_v = action_ranges[0][1] - action_ranges[0][0]
        width_omega = action_ranges[1][1] - action_ranges[1][0]
        
        if is_safe:
            safe_count += 1
        
        result = {
            'step': step_idx,
            'pose': pose.tolist(),  # â† æ–°å¢ï¼šä¿å­˜ä½å§¿ç”¨äºè°ƒè¯•
            'det_action': det_action.tolist(),
            'action_ranges': action_ranges,
            'is_safe': is_safe,
            'width_v': float(width_v),
            'width_omega': float(width_omega),
            'min_laser': float(np.min(state[:20])),
            'distance': float(state[20]),
        }
        results.append(result)
    
    # ç»Ÿè®¡éƒ¨åˆ†ä¸å˜...
    elapsed_time = time.time() - start_time
    trajectory_summary = {
        'trajectory_idx': trajectory_idx,
        'n_samples': n_samples,
        'safe_count': safe_count,
        'safety_rate': safe_count / n_samples if n_samples > 0 else 0,
        'collision': trajectory_data['collision'],
        'goal_reached': trajectory_data['goal_reached'],
        'steps': trajectory_data['steps'],
        'total_reward': float(trajectory_data['total_reward']),
        'compute_time': elapsed_time,
        'results': results,
    }
    
    print(f"[è¿›ç¨‹ {trajectory_idx+1}] âœ… å®Œæˆï¼å®‰å…¨ç‡: {trajectory_summary['safety_rate']*100:.1f}% | "
          f"è€—æ—¶: {elapsed_time/60:.1f}åˆ†é’Ÿ")
    
    return (trajectory_idx, trajectory_summary)


def main():
    """ä¸»å‡½æ•°ï¼šå¹¶è¡ŒéªŒè¯æ‰€æœ‰è½¨è¿¹"""
    print("\n" + "="*70)
    print("ğŸš€ å¹¶è¡Œå¯è¾¾æ€§éªŒè¯å·¥å…·")
    print("="*70)
    
    # ===== 1. æ£€æµ‹ CPU æ ¸å¿ƒæ•° =====
    n_cores = cpu_count()
    print(f"\næ£€æµ‹åˆ° CPU æ ¸å¿ƒæ•°: {n_cores}")
    
    # ===== 2. åŠ è½½è½¨è¿¹ =====
    print("\n[1/3] åŠ è½½ä¿å­˜çš„è½¨è¿¹...")
    trajectories = load_trajectories()
    n_trajectories = len(trajectories)
    print(f"  âœ… åŠ è½½ {n_trajectories} æ¡è½¨è¿¹")
    
    total_states = sum(t['steps'] for t in trajectories)
    print(f"  æ€»çŠ¶æ€æ•°: {total_states}")
    print(f"  å¹³å‡é•¿åº¦: {total_states/n_trajectories:.1f} æ­¥/è½¨è¿¹")
    
    # ===== 3. å‡†å¤‡å¹¶è¡Œå‚æ•° =====
    print("\n[2/3] å‡†å¤‡å¹¶è¡Œè®¡ç®—...")
    
    model_path = project_root / "models" / "TD3" / "Nov17_06-22-08_archived"
    observation_error = 0.01
    sample_interval = 10  # ä¿æŒåŸå§‹é‡‡æ ·é—´éš”
    
    # å†³å®šå¹¶è¡Œè¿›ç¨‹æ•°
    n_workers = min(n_trajectories, n_cores // 2)  # æ¯ä¸ªè¿›ç¨‹ç”¨ 2 æ ¸
    print(f"  å¹¶è¡Œè¿›ç¨‹æ•°: {n_workers}")
    print(f"  è§‚æµ‹è¯¯å·®: Â±{observation_error}")
    print(f"  é‡‡æ ·é—´éš”: æ¯ {sample_interval} æ­¥")
    print(f"  Bernstein é˜¶æ•°: 1")
    print(f"  Bernstein é‡‡æ ·: 4000 æ­¥ï¼ˆä¿æŒåŸå§‹ç²¾åº¦ï¼‰")
    
    # æ„é€ å‚æ•°åˆ—è¡¨
    args_list = [
        (i, traj, model_path, observation_error, sample_interval)
        for i, traj in enumerate(trajectories)
    ]
    
    # ===== 4. å¹¶è¡Œæ‰§è¡Œ =====
    print(f"\n[3/3] å¯åŠ¨ {n_workers} ä¸ªå¹¶è¡Œè¿›ç¨‹...")
    print("="*70)
    
    start_time = time.time()
    
    # ä½¿ç”¨è¿›ç¨‹æ± ï¼Œæ·»åŠ å¼‚å¸¸å¤„ç†
    try:
        with Pool(processes=n_workers) as pool:
            # map ä¼šè‡ªåŠ¨åˆ†é…ä»»åŠ¡åˆ°å„ä¸ªè¿›ç¨‹
            results = pool.map(verify_single_trajectory_worker, args_list)
    except Exception as e:
        print(f"\nâŒ å¹¶è¡ŒéªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("å°è¯•ä¿å­˜å·²å®Œæˆçš„éƒ¨åˆ†ç»“æœ...")
        import traceback
        traceback.print_exc()
        raise
    
    total_elapsed = time.time() - start_time
    
    # ===== 5. æ±‡æ€»ç»“æœ =====
    print("\n" + "="*70)
    print("éªŒè¯ç»Ÿè®¡:")
    print("="*70)
    
    # æŒ‰è½¨è¿¹ç´¢å¼•æ’åº
    results = sorted(results, key=lambda x: x[0])
    all_results = [r[1] for r in results]
    
    # 5.1 æ•´ä½“ç»Ÿè®¡
    total_samples = sum(r['n_samples'] for r in all_results)
    total_safe = sum(r['safe_count'] for r in all_results)
    overall_safety_rate = total_safe / total_samples if total_samples > 0 else 0
    
    print(f"\næ•´ä½“å¯è¾¾é›†å®‰å…¨æ€§:")
    print(f"  æ€»é‡‡æ ·ç‚¹: {total_samples}")
    print(f"  å®‰å…¨ç‚¹æ•°: {total_safe}")
    print(f"  å®‰å…¨ç‡: {overall_safety_rate*100:.1f}%")
    
    # 5.2 è½¨è¿¹åˆ†ç±»ç»Ÿè®¡
    goal_trajectories = [r for r in all_results if r['goal_reached']]
    collision_trajectories = [r for r in all_results if r['collision']]
    
    print(f"\næŒ‰è½¨è¿¹ç»“æœåˆ†ç±»:")
    print(f"  åˆ°è¾¾ç›®æ ‡çš„è½¨è¿¹: {len(goal_trajectories)}")
    if goal_trajectories:
        goal_safety = np.mean([r['safety_rate'] for r in goal_trajectories])
        print(f"    å¹³å‡å®‰å…¨ç‡: {goal_safety*100:.1f}%")
    
    print(f"  ç¢°æ’çš„è½¨è¿¹: {len(collision_trajectories)}")
    if collision_trajectories:
        collision_safety = np.mean([r['safety_rate'] for r in collision_trajectories])
        print(f"    å¹³å‡å®‰å…¨ç‡: {collision_safety*100:.1f}%")
    
    # 5.3 å¯è¾¾é›†å®½åº¦ç»Ÿè®¡
    all_widths_v = []
    all_widths_omega = []
    
    for result in all_results:
        for r in result['results']:
            all_widths_v.append(r['width_v'])
            all_widths_omega.append(r['width_omega'])
    
    print(f"\nå¯è¾¾é›†å®½åº¦ç»Ÿè®¡:")
    print(f"  çº¿é€Ÿåº¦:")
    print(f"    å¹³å‡: {np.mean(all_widths_v):.6f}")
    print(f"    æ ‡å‡†å·®: {np.std(all_widths_v):.6f}")
    print(f"    æœ€å¤§: {np.max(all_widths_v):.6f}")
    
    print(f"  è§’é€Ÿåº¦:")
    print(f"    å¹³å‡: {np.mean(all_widths_omega):.6f}")
    print(f"    æ ‡å‡†å·®: {np.std(all_widths_omega):.6f}")
    print(f"    æœ€å¤§: {np.max(all_widths_omega):.6f}")
    
    # 5.4 æ€§èƒ½ç»Ÿè®¡
    print(f"\næ€§èƒ½ç»Ÿè®¡:")
    print(f"  æ€»è€—æ—¶: {total_elapsed/60:.1f} åˆ†é’Ÿ ({total_elapsed/3600:.2f} å°æ—¶)")
    print(f"  å¹³å‡æ¯è½¨è¿¹: {total_elapsed/n_trajectories:.1f} ç§’")
    print(f"  å¹³å‡æ¯é‡‡æ ·ç‚¹: {total_elapsed/total_samples:.2f} ç§’")
    
    # è®¡ç®—åŠ é€Ÿæ¯”
    avg_traj_time = np.mean([r['compute_time'] for r in all_results])
    serial_time = avg_traj_time * n_trajectories
    speedup = serial_time / total_elapsed
    
    print(f"\nå¹¶è¡ŒåŠ é€Ÿ:")
    print(f"  ä¸²è¡Œé¢„è®¡è€—æ—¶: {serial_time/60:.1f} åˆ†é’Ÿ ({serial_time/3600:.2f} å°æ—¶)")
    print(f"  å¹¶è¡Œå®é™…è€—æ—¶: {total_elapsed/60:.1f} åˆ†é’Ÿ ({total_elapsed/3600:.2f} å°æ—¶)")
    print(f"  åŠ é€Ÿæ¯”: {speedup:.1f}x")
    print(f"  å¹¶è¡Œæ•ˆç‡: {speedup/n_workers*100:.1f}%")
    
    # ===== 6. ä¿å­˜ç»“æœ =====
    output_path = Path(__file__).parent.parent / "assets" / "reachability_results_parallel.json"
    
    output_data = {
        'metadata': {
            'n_trajectories': n_trajectories,
            'total_samples': total_samples,
            'observation_error': observation_error,
            'sample_interval': sample_interval,
            'bern_order': 1,
            'error_steps': 4000,
            'n_workers': n_workers,
            'n_cores': n_cores,
            'elapsed_time': total_elapsed,
            'speedup': speedup,
        },
        'summary': {
            'overall_safety_rate': overall_safety_rate,
            'total_safe': total_safe,
            'total_samples': total_samples,
            'goal_trajectories': len(goal_trajectories),
            'collision_trajectories': len(collision_trajectories),
        },
        'trajectories': all_results,
    }
    
    try:
        with open(output_path, 'w') as f:
            json.dump(output_data, f, indent=2)
        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        print(f"   æ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024:.1f} KB")
    except Exception as e:
        print(f"\nâŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
        print(f"   ç›®æ ‡è·¯å¾„: {output_path}")
        import traceback
        traceback.print_exc()
        raise
    
    print("="*70)
    print("\nğŸ‰ å¹¶è¡ŒéªŒè¯å®Œæˆï¼")


if __name__ == "__main__":
    main()