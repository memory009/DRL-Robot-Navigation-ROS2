#!/usr/bin/env python3
"""
å¹¶è¡Œå¯è¾¾æ€§éªŒè¯è„šæœ¬ - çº¯POLARç‰ˆæœ¬ (TD3_lightweight)
ç§»é™¤å…‰çº¿æŠ•å°„ï¼Œå®Œå…¨éµå¾ªè®ºæ–‡æ–¹æ³•
ä½¿ç”¨è½»é‡çº§ç½‘ç»œåŠ é€Ÿè®¡ç®—
"""

import sys
try:
    import distutils.version
except AttributeError:
    import distutils
    from packaging import version as packaging_version
    distutils.version = type('version', (), {
        'LooseVersion': packaging_version.Version,
        'StrictVersion': packaging_version.Version
    })
from pathlib import Path
import numpy as np
import torch
import pickle
import json
import time
from multiprocessing import Pool, cpu_count

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# â† ä¿®æ”¹1: å¯¼å…¥è½»é‡çº§TD3
from TD3.TD3_lightweight import TD3 as TD3_Lightweight


def compute_reachable_set_pure_polar(
    actor,
    state,
    observation_error=0.01,
    bern_order=1,
    error_steps=4000,
    max_action=1.0,
):
    """
    çº¯POLARå¯è¾¾æ€§éªŒè¯ - ä¸clearpath_rl_polarå®Œå…¨ä¸€è‡´
    æ”¯æŒåŠ¨æ€ç½‘ç»œç»“æ„ï¼ˆè‡ªåŠ¨é€‚é…éšè—å±‚ç»´åº¦ï¼‰
    """
    import sympy as sym
    from verification.taylor_model import (
        TaylorModel,
        TaylorArithmetic,
        BernsteinPolynomial,
        compute_tm_bounds,
        apply_activation,
    )
    
    # 1. æå–Actoræƒé‡
    weights = []
    biases = []
    
    with torch.no_grad():
        for name, param in actor.named_parameters():
            if 'weight' in name:
                weights.append(param.cpu().numpy())
            elif 'bias' in name:
                biases.append(param.cpu().numpy())
    
    # éªŒè¯ç½‘ç»œç»“æ„
    state_dim = len(state)
    assert weights[0].shape[1] == state_dim, \
        f"è¾“å…¥ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {state_dim}, å®é™… {weights[0].shape[1]}"
    assert weights[-1].shape[0] == 2, \
        f"è¾“å‡ºç»´åº¦ä¸åŒ¹é…: æœŸæœ› 2, å®é™… {weights[-1].shape[0]}"
    
    # è‡ªåŠ¨æ£€æµ‹éšè—å±‚ç»´åº¦
    hidden_dim = weights[0].shape[0]
    # print(f"[POLAR] æ£€æµ‹åˆ°ç½‘ç»œç»“æ„: {state_dim} â†’ {hidden_dim} â†’ {hidden_dim} â†’ 2")
    
    # 2. åˆ›å»ºç¬¦å·å˜é‡
    z_symbols = [sym.Symbol(f'z{i}') for i in range(state_dim)]
    
    # 3. æ„é€ è¾“å…¥Tayloræ¨¡å‹
    TM_state = []
    for i in range(state_dim):
        poly = sym.Poly(
            observation_error * z_symbols[i] + state[i], 
            *z_symbols
        )
        TM_state.append(TaylorModel(poly, [0.0, 0.0]))
    
    # 4. é€å±‚ä¼ æ’­
    TM_input = TM_state
    TA = TaylorArithmetic()
    BP = BernsteinPolynomial(error_steps=error_steps)
    
    num_layers = len(biases)
    
    for layer_idx in range(num_layers):
        TM_temp = []
        W = weights[layer_idx]
        b = biases[layer_idx]
        
        num_neurons = len(b)
        
        for neuron_idx in range(num_neurons):
            # çº¿æ€§å˜æ¢
            tm_neuron = TA.weighted_sumforall(
                TM_input,
                W[neuron_idx],
                b[neuron_idx]
            )
            
            # æ¿€æ´»å‡½æ•°
            is_hidden = (layer_idx < num_layers - 1)
            
            if is_hidden:
                # ReLU (ä½¿ç”¨è®ºæ–‡Equation 8çš„ä¼˜åŒ–)
                a, b_bound = compute_tm_bounds(tm_neuron)
                
                if a >= 0:
                    # æƒ…å†µ1: å®Œå…¨æ¿€æ´»
                    TM_after = tm_neuron
                elif b_bound <= 0:
                    # æƒ…å†µ2: å®Œå…¨ä¸æ¿€æ´»
                    zero_poly = sym.Poly(0, *z_symbols)
                    TM_after = TaylorModel(zero_poly, [0, 0])
                else:
                    # æƒ…å†µ3: è·¨è¶Šé›¶ç‚¹ï¼Œä½¿ç”¨Bernsteinå¤šé¡¹å¼
                    bern_poly = BP.approximate(a, b_bound, bern_order, 'relu')
                    bern_error = BP.compute_error(a, b_bound, 'relu')
                    TM_after = apply_activation(
                        tm_neuron, bern_poly, bern_error, bern_order
                    )
            else:
                # è¾“å‡ºå±‚: Tanh
                a, b_bound = compute_tm_bounds(tm_neuron)
                bern_poly = BP.approximate(a, b_bound, bern_order, 'tanh')
                bern_error = BP.compute_error(a, b_bound, 'tanh')
                TM_after = apply_activation(
                    tm_neuron, bern_poly, bern_error, bern_order
                )
                # ç¼©æ”¾åˆ°åŠ¨ä½œç©ºé—´
                TM_after = TA.constant_product(TM_after, max_action)
            
            TM_temp.append(TM_after)
        
        TM_input = TM_temp
    
    # 5. è®¡ç®—åŠ¨ä½œå¯è¾¾é›†
    action_ranges = []
    for tm in TM_input:
        a, b = compute_tm_bounds(tm)
        action_ranges.append([a, b])
    
    return action_ranges


def check_action_safety_simple(action_ranges, state):
    """
    ç®€å•çš„å®‰å…¨æ€§æ£€æŸ¥ - ä¸clearpath_rl_polarä¸€è‡´
    åªåŸºäºå¯è¾¾é›†å®½åº¦å’Œæ¿€å…‰é›·è¾¾æ•°æ®
    """
    # 1. æ£€æŸ¥å¯è¾¾é›†å®½åº¦
    for i, (min_val, max_val) in enumerate(action_ranges):
        range_width = max_val - min_val
        if range_width > 1.5:
            return False
    
    # 2. æ£€æŸ¥ç¢°æ’é£é™©ï¼ˆåŸºäºæ¿€å…‰é›·è¾¾ï¼‰
    laser_readings = state[2:10]  # 8ä¸ªæ¿€å…‰æ•°æ®ï¼ˆå·²å½’ä¸€åŒ–ï¼‰
    min_laser = np.min(laser_readings)
    
    if min_laser < 0.05:  # å¾ˆè¿‘çš„éšœç¢ç‰©
        linear_vel_range = action_ranges[0]
        if linear_vel_range[1] > 0.3:  # å¯èƒ½å‰è¿›
            return False
    
    # 3. æ£€æŸ¥åŠ¨ä½œèŒƒå›´
    if action_ranges[0][0] < -0.6 or action_ranges[0][1] > 0.6:
        return False
    if action_ranges[1][0] < -1.1 or action_ranges[1][1] > 1.1:
        return False
    
    return True


def verify_single_trajectory_worker(args):
    """å•ä¸ªè½¨è¿¹çš„éªŒè¯å‡½æ•°ï¼ˆçº¯POLAR + lightweightç‰ˆæœ¬ï¼‰"""
    trajectory_idx, trajectory_data, model_path, observation_error, sample_interval = args
    
    # â† ä¿®æ”¹2: åŠ è½½è½»é‡çº§æ¨¡å‹
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    agent = TD3_Lightweight(
        state_dim=25,
        action_dim=2,
        max_action=1.0,
        device=device,
        hidden_dim=26,  # â† è½»é‡çº§ç½‘ç»œçš„éšè—å±‚ç»´åº¦
        load_model=True,
        model_name="TD3_lightweight_best",  # â† è½»é‡çº§æ¨¡å‹åç§°
        load_directory=model_path,
    )
    
    # æå–çŠ¶æ€å¹¶é‡‡æ ·
    states = trajectory_data['states']
    poses = trajectory_data['poses']
    
    sampled_states = states[::sample_interval]
    sampled_poses = poses[::sample_interval]
    n_samples = len(sampled_states)
    
    print(f"[è¿›ç¨‹ {trajectory_idx+1}] å¼€å§‹éªŒè¯ {n_samples} ä¸ªé‡‡æ ·ç‚¹ï¼ˆçº¯POLAR-Lightweightï¼‰...")
    
    results = []
    safe_count = 0
    start_time = time.time()
    
    for i, (state, pose) in enumerate(zip(sampled_states, sampled_poses)):
        step_idx = i * sample_interval
        
        if i % max(1, n_samples // 4) == 0:
            elapsed = time.time() - start_time
            print(f"[è¿›ç¨‹ {trajectory_idx+1}] è¿›åº¦: {i+1}/{n_samples} "
                  f"({i/n_samples*100:.0f}%) | å·²ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ")
        
        # çº¯POLARè®¡ç®—å¯è¾¾é›†
        action_ranges = compute_reachable_set_pure_polar(
            agent.actor,
            state,
            observation_error=observation_error,
            bern_order=1,
            error_steps=4000,
            max_action=1.0,
        )
        
        # ç®€å•å®‰å…¨æ€§æ£€æŸ¥
        is_safe = check_action_safety_simple(action_ranges, state)
        
        det_action = agent.get_action(state, add_noise=False)
        width_v = action_ranges[0][1] - action_ranges[0][0]
        width_omega = action_ranges[1][1] - action_ranges[1][0]
        
        if is_safe:
            safe_count += 1
        
        result = {
            'step': step_idx,
            'pose': pose.tolist(),
            'det_action': det_action.tolist(),
            'action_ranges': action_ranges,
            'is_safe': is_safe,
            'width_v': float(width_v),
            'width_omega': float(width_omega),
            'min_laser': float(np.min(state[:20])),
            'distance': float(state[20]),
        }
        results.append(result)
    
    elapsed_time = time.time() - start_time
    trajectory_summary = {
        'trajectory_idx': trajectory_idx,
        'n_samples': n_samples,
        'safe_count': safe_count,
        'safety_rate': safe_count / n_samples if n_samples > 0 else 0,
        'collision': trajectory_data['collision'],
        'goal_reached': trajectory_data['goal_reached'],
        'steps': trajectory_data['steps'],
        'total_reward': float(trajectory_data['total_reward']),
        'compute_time': elapsed_time,
        'results': results,
    }
    
    print(f"[è¿›ç¨‹ {trajectory_idx+1}] âœ… å®Œæˆï¼å®‰å…¨ç‡: {trajectory_summary['safety_rate']*100:.1f}% | "
          f"è€—æ—¶: {elapsed_time/60:.1f}åˆ†é’Ÿ")
    
    return (trajectory_idx, trajectory_summary)


def load_trajectories(pkl_path=None):
    """åŠ è½½ä¿å­˜çš„è½¨è¿¹"""
    if pkl_path is None:
        # â† ä¿®æ”¹3: ä½¿ç”¨lightweightçš„è½¨è¿¹æ–‡ä»¶
        pkl_path = Path(__file__).parent.parent / "assets" / "trajectories_lightweight.pkl"
    
    if not pkl_path.exists():
        raise FileNotFoundError(f"è½¨è¿¹æ–‡ä»¶ä¸å­˜åœ¨: {pkl_path}")
    
    with open(pkl_path, 'rb') as f:
        trajectories = pickle.load(f)
    
    valid_trajectories = [t for t in trajectories if t is not None]
    return valid_trajectories


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print("ğŸš€ çº¯POLARå¹¶è¡ŒéªŒè¯å·¥å…· (TD3_Lightweight)")
    print("="*70)
    
    n_cores = cpu_count()
    print(f"\næ£€æµ‹åˆ° CPU æ ¸å¿ƒæ•°: {n_cores}")
    
    print("\n[1/3] åŠ è½½è½¨è¿¹...")
    trajectories = load_trajectories()
    n_trajectories = len(trajectories)
    print(f"  âœ… åŠ è½½ {n_trajectories} æ¡è½¨è¿¹")
    
    total_states = sum(t['steps'] for t in trajectories)
    print(f"  æ€»çŠ¶æ€æ•°: {total_states}")
    
    print("\n[2/3] å‡†å¤‡å¹¶è¡Œè®¡ç®—...")
    
    # â† ä¿®æ”¹4: ä½¿ç”¨lightweightæ¨¡å‹è·¯å¾„
    model_path = project_root / "models" / "TD3_lightweight" / "Nov19_01-37-30_cheeson"
    observation_error = 0.01
    sample_interval = 1
    
    n_workers = min(n_trajectories, n_cores // 2)
    print(f"  æ¨¡å‹: TD3_Lightweight (26ç¥ç»å…ƒ)")
    print(f"  æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"  å¹¶è¡Œè¿›ç¨‹æ•°: {n_workers}")
    print(f"  è§‚æµ‹è¯¯å·®: Â±{observation_error}")
    print(f"  é‡‡æ ·é—´éš”: æ¯ {sample_interval} æ­¥")
    
    args_list = [
        (i, traj, model_path, observation_error, sample_interval)
        for i, traj in enumerate(trajectories)
    ]
    
    print(f"\n[3/3] å¯åŠ¨ {n_workers} ä¸ªå¹¶è¡Œè¿›ç¨‹...")
    print("="*70)
    
    start_time = time.time()
    
    try:
        with Pool(processes=n_workers) as pool:
            results = pool.map(verify_single_trajectory_worker, args_list)
    except Exception as e:
        print(f"\nâŒ å¹¶è¡ŒéªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise
    
    total_elapsed = time.time() - start_time
    
    print("\n" + "="*70)
    print("éªŒè¯ç»Ÿè®¡:")
    print("="*70)
    
    results = sorted(results, key=lambda x: x[0])
    all_results = [r[1] for r in results]
    
    total_samples = sum(r['n_samples'] for r in all_results)
    total_safe = sum(r['safe_count'] for r in all_results)
    overall_safety_rate = total_safe / total_samples if total_samples > 0 else 0
    
    print(f"\næ•´ä½“å¯è¾¾é›†å®‰å…¨æ€§:")
    print(f"  æ€»é‡‡æ ·ç‚¹: {total_samples}")
    print(f"  å®‰å…¨ç‚¹æ•°: {total_safe}")
    print(f"  å®‰å…¨ç‡: {overall_safety_rate*100:.1f}%")
    
    # è½¨è¿¹åˆ†ç±»ç»Ÿè®¡
    goal_trajectories = [r for r in all_results if r['goal_reached']]
    collision_trajectories = [r for r in all_results if r['collision']]
    
    print(f"\næŒ‰è½¨è¿¹ç»“æœåˆ†ç±»:")
    print(f"  åˆ°è¾¾ç›®æ ‡çš„è½¨è¿¹: {len(goal_trajectories)}")
    if goal_trajectories:
        goal_safety = np.mean([r['safety_rate'] for r in goal_trajectories])
        print(f"    å¹³å‡å®‰å…¨ç‡: {goal_safety*100:.1f}%")
    
    print(f"  ç¢°æ’çš„è½¨è¿¹: {len(collision_trajectories)}")
    if collision_trajectories:
        collision_safety = np.mean([r['safety_rate'] for r in collision_trajectories])
        print(f"    å¹³å‡å®‰å…¨ç‡: {collision_safety*100:.1f}%")
    
    # å¯è¾¾é›†å®½åº¦ç»Ÿè®¡
    all_widths_v = []
    all_widths_omega = []
    
    for result in all_results:
        for r in result['results']:
            all_widths_v.append(r['width_v'])
            all_widths_omega.append(r['width_omega'])
    
    print(f"\nå¯è¾¾é›†å®½åº¦ç»Ÿè®¡:")
    print(f"  çº¿é€Ÿåº¦:")
    print(f"    å¹³å‡: {np.mean(all_widths_v):.6f}")
    print(f"    æ ‡å‡†å·®: {np.std(all_widths_v):.6f}")
    print(f"    æœ€å¤§: {np.max(all_widths_v):.6f}")
    
    print(f"  è§’é€Ÿåº¦:")
    print(f"    å¹³å‡: {np.mean(all_widths_omega):.6f}")
    print(f"    æ ‡å‡†å·®: {np.std(all_widths_omega):.6f}")
    print(f"    æœ€å¤§: {np.max(all_widths_omega):.6f}")
    
    print(f"\næ€§èƒ½ç»Ÿè®¡:")
    print(f"  æ€»è€—æ—¶: {total_elapsed/60:.1f} åˆ†é’Ÿ ({total_elapsed/3600:.2f} å°æ—¶)")
    print(f"  å¹³å‡æ¯è½¨è¿¹: {total_elapsed/n_trajectories:.1f} ç§’")
    print(f"  å¹³å‡æ¯é‡‡æ ·ç‚¹: {total_elapsed/total_samples:.2f} ç§’")
    
    # è®¡ç®—åŠ é€Ÿæ¯”
    avg_traj_time = np.mean([r['compute_time'] for r in all_results])
    serial_time = avg_traj_time * n_trajectories
    speedup = serial_time / total_elapsed
    
    print(f"\nå¹¶è¡ŒåŠ é€Ÿ:")
    print(f"  ä¸²è¡Œé¢„è®¡è€—æ—¶: {serial_time/60:.1f} åˆ†é’Ÿ")
    print(f"  å¹¶è¡Œå®é™…è€—æ—¶: {total_elapsed/60:.1f} åˆ†é’Ÿ")
    print(f"  åŠ é€Ÿæ¯”: {speedup:.1f}x")
    print(f"  å¹¶è¡Œæ•ˆç‡: {speedup/n_workers*100:.1f}%")
    
    # â† ä¿®æ”¹5: ä¿å­˜åˆ°lightweightä¸“ç”¨æ–‡ä»¶
    output_path = Path(__file__).parent.parent / "assets" / "reachability_results_pure_polar_lightweight_ori.json"
    
    output_data = {
        'metadata': {
            'method': 'pure_polar',
            'model': 'TD3_lightweight',
            'hidden_dim': 26,
            'n_trajectories': n_trajectories,
            'total_samples': total_samples,
            'observation_error': observation_error,
            'sample_interval': sample_interval,
            'bern_order': 1,
            'error_steps': 4000,
            'n_workers': n_workers,
            'n_cores': n_cores,
            'elapsed_time': total_elapsed,
            'speedup': speedup,
        },
        'summary': {
            'overall_safety_rate': overall_safety_rate,
            'total_safe': total_safe,
            'total_samples': total_samples,
            'goal_trajectories': len(goal_trajectories),
            'collision_trajectories': len(collision_trajectories),
        },
        'trajectories': all_results,
    }
    
    try:
        with open(output_path, 'w') as f:
            json.dump(output_data, f, indent=2)
        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        print(f"   æ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024:.1f} KB")
    except Exception as e:
        print(f"\nâŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise
    
    print("="*70)
    print("\nğŸ‰ çº¯POLARéªŒè¯å®Œæˆï¼ˆLightweightç‰ˆæœ¬ï¼‰ï¼")
    print(f"ğŸ’¡ æç¤º: è½»é‡çº§ç½‘ç»œå‚æ•°é‡å‡å°‘çº¦99.8%ï¼Œè®¡ç®—é€Ÿåº¦æ›´å¿«")


if __name__ == "__main__":
    main()