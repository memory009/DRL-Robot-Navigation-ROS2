#!/usr/bin/env python3
"""
å¹¶è¡Œå¯è¾¾æ€§éªŒè¯è„šæœ¬ - çº¯POLARç‰ˆæœ¬ (TD3_lightweight) - ä¿®æ­£ç‰ˆ
ä¸è®­ç»ƒä»£ç å®Œå…¨å¯¹é½
"""

import sys
try:
    import distutils.version
except AttributeError:
    import distutils
    from packaging import version as packaging_version
    distutils.version = type('version', (), {
        'LooseVersion': packaging_version.Version,
        'StrictVersion': packaging_version.Version
    })
from pathlib import Path
import numpy as np
import torch
import pickle
import json
import time
from multiprocessing import Pool, cpu_count

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from TD3.TD3_lightweight import TD3 as TD3_Lightweight


def compute_reachable_set_pure_polar(
    actor,
    state,
    observation_error=0.01,
    bern_order=1,
    error_steps=4000,
    max_action=1.0,
):
    """
    çº¯POLARå¯è¾¾æ€§éªŒè¯ - ä¸clearpath_rl_polarå®Œå…¨ä¸€è‡´
    æ”¯æŒåŠ¨æ€ç½‘ç»œç»“æ„ï¼ˆè‡ªåŠ¨é€‚é…éšè—å±‚ç»´åº¦ï¼‰
    """
    import sympy as sym
    from verification.taylor_model import (
        TaylorModel,
        TaylorArithmetic,
        BernsteinPolynomial,
        compute_tm_bounds,
        apply_activation,
    )
    
    # 1. æå–Actoræƒé‡
    weights = []
    biases = []
    
    with torch.no_grad():
        for name, param in actor.named_parameters():
            if 'weight' in name:
                weights.append(param.cpu().numpy())
            elif 'bias' in name:
                biases.append(param.cpu().numpy())
    
    # éªŒè¯ç½‘ç»œç»“æ„
    state_dim = len(state)
    assert weights[0].shape[1] == state_dim, \
        f"è¾“å…¥ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {state_dim}, å®é™… {weights[0].shape[1]}"
    assert weights[-1].shape[0] == 2, \
        f"è¾“å‡ºç»´åº¦ä¸åŒ¹é…: æœŸæœ› 2, å®é™… {weights[-1].shape[0]}"
    
    # è‡ªåŠ¨æ£€æµ‹éšè—å±‚ç»´åº¦
    hidden_dim = weights[0].shape[0]
    
    # 2. åˆ›å»ºç¬¦å·å˜é‡
    z_symbols = [sym.Symbol(f'z{i}') for i in range(state_dim)]
    
    # 3. æ„é€ è¾“å…¥Tayloræ¨¡å‹
    TM_state = []
    for i in range(state_dim):
        poly = sym.Poly(
            observation_error * z_symbols[i] + state[i], 
            *z_symbols
        )
        TM_state.append(TaylorModel(poly, [0.0, 0.0]))
    
    # 4. é€å±‚ä¼ æ’­
    TM_input = TM_state
    TA = TaylorArithmetic()
    BP = BernsteinPolynomial(error_steps=error_steps)
    
    num_layers = len(biases)
    
    for layer_idx in range(num_layers):
        TM_temp = []
        W = weights[layer_idx]
        b = biases[layer_idx]
        
        num_neurons = len(b)
        
        for neuron_idx in range(num_neurons):
            # çº¿æ€§å˜æ¢
            tm_neuron = TA.weighted_sumforall(
                TM_input,
                W[neuron_idx],
                b[neuron_idx]
            )
            
            # æ¿€æ´»å‡½æ•°
            is_hidden = (layer_idx < num_layers - 1)
            
            if is_hidden:
                # ReLU (ä½¿ç”¨è®ºæ–‡Equation 8çš„ä¼˜åŒ–)
                a, b_bound = compute_tm_bounds(tm_neuron)
                
                if a >= 0:
                    # æƒ…å†µ1: å®Œå…¨æ¿€æ´»
                    TM_after = tm_neuron
                elif b_bound <= 0:
                    # æƒ…å†µ2: å®Œå…¨ä¸æ¿€æ´»
                    zero_poly = sym.Poly(0, *z_symbols)
                    TM_after = TaylorModel(zero_poly, [0, 0])
                else:
                    # æƒ…å†µ3: è·¨è¶Šé›¶ç‚¹ï¼Œä½¿ç”¨Bernsteinå¤šé¡¹å¼
                    bern_poly = BP.approximate(a, b_bound, bern_order, 'relu')
                    bern_error = BP.compute_error(a, b_bound, 'relu')
                    TM_after = apply_activation(
                        tm_neuron, bern_poly, bern_error, bern_order
                    )
            else:
                # è¾“å‡ºå±‚: Tanh
                a, b_bound = compute_tm_bounds(tm_neuron)
                bern_poly = BP.approximate(a, b_bound, bern_order, 'tanh')
                bern_error = BP.compute_error(a, b_bound, 'tanh')
                TM_after = apply_activation(
                    tm_neuron, bern_poly, bern_error, bern_order
                )
                # ç¼©æ”¾åˆ°åŠ¨ä½œç©ºé—´
                TM_after = TA.constant_product(TM_after, max_action)
            
            TM_temp.append(TM_after)
        
        TM_input = TM_temp
    
    # 5. è®¡ç®—åŠ¨ä½œå¯è¾¾é›†
    action_ranges = []
    for tm in TM_input:
        a, b = compute_tm_bounds(tm)
        action_ranges.append([a, b])
    
    return action_ranges


def check_action_safety_training_aligned(action_ranges, state):
    """
    âœ… ä¸è®ºæ–‡å’Œä½œè€…ä»£ç å®Œå…¨å¯¹é½çš„å®‰å…¨æ£€æŸ¥
    
    åŸºäºä½œè€… main.py çš„åŒºé—´è®¡ç®—é€»è¾‘ï¼š
    - ä½¿ç”¨ Taylor Model ä¸Šç•Œä½œä¸ºä¿å®ˆä¼°è®¡
    - åŒºé—´ç®—æœ¯è®¡ç®—æœ€åæƒ…å†µä½ç§»
    - åŸºäºå½“å‰æ¿€å…‰æ£€æŸ¥ç¢°æ’é£é™©
    
    å‚è€ƒï¼š
    - ä½œè€…ä»£ç ï¼šmain.py (Line 14-26)
    - è®ºæ–‡ï¼šSection IV-A, Fig. 2, Remark 1
    """
    # ===== å‚æ•°ï¼ˆä¸è®­ç»ƒä»£ç å¯¹é½ï¼‰=====
    COLLISION_DELTA = 0.4      # ros_python.py: check_collision()
    SAFETY_MARGIN = 0.05       # éªŒè¯æ—¶çš„ä¿å®ˆè£•åº¦
    DT = 0.1                   # ros_python.py: time.sleep(0.1)
    
    # ===== 1. æå–ç¯å¢ƒä¿¡æ¯ =====
    laser_readings = state[0:20]
    min_laser = np.min(laser_readings)
    
    # ===== 2. åŠ¨ä½œå¯è¾¾é›†ï¼ˆPOLARè®¡ç®—çš„åŒºé—´ï¼‰=====
    v_interval = action_ranges[0]  # [v_min, v_max]
    
    # ===== 3. æ˜ å°„åˆ°å®é™…æ§åˆ¶ç©ºé—´ =====
    # å¯¹åº”è®­ç»ƒä»£ç ï¼ša_in[0] = (action[0] + 1) / 2
    v_actual_max = (v_interval[1] + 1) / 2
    
    # ===== 4. ä¿å®ˆä¼°è®¡ï¼šæœ€åæƒ…å†µä½ç§»ï¼ˆä½¿ç”¨ä¸Šç•Œï¼‰=====
    # å¯¹åº”ä½œè€…ä»£ç ï¼šb = constant + sum(newlist[0:-1]) + TMI_temp[1]
    max_displacement = v_actual_max * DT
    
    # ===== 5. ç¢°æ’æ£€æŸ¥ =====
    predicted_min_distance = min_laser - max_displacement
    safe_threshold = COLLISION_DELTA + SAFETY_MARGIN
    
    if predicted_min_distance < safe_threshold:
        return False  # ä¸å®‰å…¨ï¼šå¯è¾¾é›†å¯èƒ½å¯¼è‡´ç¢°æ’
    
    return True  # å®‰å…¨ï¼šæ‰€æœ‰å¯èƒ½è½¨è¿¹éƒ½ä¸ä¼šç¢°æ’

def verify_single_trajectory_worker(args):
    """å•ä¸ªè½¨è¿¹çš„éªŒè¯å‡½æ•°ï¼ˆçº¯POLAR + lightweightç‰ˆæœ¬ï¼‰"""
    trajectory_idx, trajectory_data, model_path, observation_error, sample_interval = args
    
    # åŠ è½½è½»é‡çº§æ¨¡å‹
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    agent = TD3_Lightweight(
        state_dim=25,
        action_dim=2,
        max_action=1.0,
        device=device,
        hidden_dim=26,
        load_model=True,
        model_name="TD3_lightweight_best",
        load_directory=model_path,
    )
    
    # æå–çŠ¶æ€å¹¶é‡‡æ ·
    states = trajectory_data['states']
    poses = trajectory_data['poses']
    
    sampled_states = states[::sample_interval]
    sampled_poses = poses[::sample_interval]
    n_samples = len(sampled_states)
    
    print(f"[è¿›ç¨‹ {trajectory_idx+1}] å¼€å§‹éªŒè¯ {n_samples} ä¸ªé‡‡æ ·ç‚¹ï¼ˆä¿®æ­£ç‰ˆï¼‰...")
    
    results = []
    safe_count = 0
    start_time = time.time()
    
    for i, (state, pose) in enumerate(zip(sampled_states, sampled_poses)):
        step_idx = i * sample_interval
        
        if i % max(1, n_samples // 4) == 0:
            elapsed = time.time() - start_time
            print(f"[è¿›ç¨‹ {trajectory_idx+1}] è¿›åº¦: {i+1}/{n_samples} "
                  f"({i/n_samples*100:.0f}%) | å·²ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ")
        
        # è®¡ç®—å¯è¾¾é›†
        action_ranges = compute_reachable_set_pure_polar(
            agent.actor,
            state,
            observation_error=observation_error,
            bern_order=1,
            error_steps=4000,
            max_action=1.0,
        )
        
        # âœ… ä½¿ç”¨ä¿®æ­£åçš„å®‰å…¨æ£€æŸ¥
        is_safe = check_action_safety_training_aligned(action_ranges, state)
        
        det_action = agent.get_action(state, add_noise=False)
        width_v = action_ranges[0][1] - action_ranges[0][0]
        width_omega = action_ranges[1][1] - action_ranges[1][0]
        
        if is_safe:
            safe_count += 1
        
        result = {
            'step': step_idx,
            'pose': pose.tolist(),
            'det_action': det_action.tolist(),
            'action_ranges': action_ranges,
            'is_safe': is_safe,
            'width_v': float(width_v),
            'width_omega': float(width_omega),
            'min_laser': float(np.min(state[0:20])),  # âœ… ä¿®æ­£ï¼šå®Œæ•´æ¿€å…‰
            'distance': float(state[20]),
        }
        results.append(result)
    
    elapsed_time = time.time() - start_time
    trajectory_summary = {
        'trajectory_idx': trajectory_idx,
        'n_samples': n_samples,
        'safe_count': safe_count,
        'safety_rate': safe_count / n_samples if n_samples > 0 else 0,
        'collision': trajectory_data['collision'],
        'goal_reached': trajectory_data['goal_reached'],
        'steps': trajectory_data['steps'],
        'total_reward': float(trajectory_data['total_reward']),
        'compute_time': elapsed_time,
        'results': results,
    }
    
    print(f"[è¿›ç¨‹ {trajectory_idx+1}] âœ… å®Œæˆï¼å®‰å…¨ç‡: {trajectory_summary['safety_rate']*100:.1f}% | "
          f"è€—æ—¶: {elapsed_time/60:.1f}åˆ†é’Ÿ")
    
    return (trajectory_idx, trajectory_summary)


def load_trajectories(pkl_path=None):
    """åŠ è½½ä¿å­˜çš„è½¨è¿¹"""
    if pkl_path is None:
        pkl_path = Path(__file__).parent.parent / "assets" / "trajectories_lightweight_12.pkl"
    
    if not pkl_path.exists():
        raise FileNotFoundError(f"è½¨è¿¹æ–‡ä»¶ä¸å­˜åœ¨: {pkl_path}")
    
    with open(pkl_path, 'rb') as f:
        trajectories = pickle.load(f)
    
    valid_trajectories = [t for t in trajectories if t is not None]
    return valid_trajectories


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print("ğŸš€ çº¯POLARå¹¶è¡ŒéªŒè¯å·¥å…· (TD3_Lightweight) - ä¿®æ­£ç‰ˆ")
    print("="*70)
    
    n_cores = cpu_count()
    print(f"\næ£€æµ‹åˆ° CPU æ ¸å¿ƒæ•°: {n_cores}")
    
    print("\n[1/3] åŠ è½½è½¨è¿¹...")
    trajectories = load_trajectories()
    n_trajectories = len(trajectories)
    print(f"  âœ… åŠ è½½ {n_trajectories} æ¡è½¨è¿¹")
    
    total_states = sum(t['steps'] for t in trajectories)
    print(f"  æ€»çŠ¶æ€æ•°: {total_states}")
    
    print("\n[2/3] å‡†å¤‡å¹¶è¡Œè®¡ç®—...")
    
    model_path = project_root / "models" / "TD3_lightweight" / "Nov24_22-43-08_cheeson"
    observation_error = 0.01
    sample_interval = 1
    
    n_workers = min(n_trajectories, n_cores // 2)
    print(f"  æ¨¡å‹: TD3_Lightweight (26ç¥ç»å…ƒ)")
    print(f"  æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"  å¹¶è¡Œè¿›ç¨‹æ•°: {n_workers}")
    print(f"  è§‚æµ‹è¯¯å·®: Â±{observation_error}")
    print(f"  é‡‡æ ·é—´éš”: æ¯ {sample_interval} æ­¥")
    print(f"  âœ… ä¿®æ­£ï¼šæ¿€å…‰ç´¢å¼• state[0:20]ï¼ŒåŠ¨ä½œæ˜ å°„ (action+1)/2ï¼Œå®½åº¦é˜ˆå€¼ 0.5/0.4")
    
    args_list = [
        (i, traj, model_path, observation_error, sample_interval)
        for i, traj in enumerate(trajectories)
    ]
    
    print(f"\n[3/3] å¯åŠ¨ {n_workers} ä¸ªå¹¶è¡Œè¿›ç¨‹...")
    print("="*70)
    
    start_time = time.time()
    
    try:
        with Pool(processes=n_workers) as pool:
            results = pool.map(verify_single_trajectory_worker, args_list)
    except Exception as e:
        print(f"\nâŒ å¹¶è¡ŒéªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise
    
    total_elapsed = time.time() - start_time
    
    print("\n" + "="*70)
    print("éªŒè¯ç»Ÿè®¡:")
    print("="*70)
    
    results = sorted(results, key=lambda x: x[0])
    all_results = [r[1] for r in results]
    
    total_samples = sum(r['n_samples'] for r in all_results)
    total_safe = sum(r['safe_count'] for r in all_results)
    overall_safety_rate = total_safe / total_samples if total_samples > 0 else 0
    
    print(f"\næ•´ä½“å¯è¾¾é›†å®‰å…¨æ€§:")
    print(f"  æ€»é‡‡æ ·ç‚¹: {total_samples}")
    print(f"  å®‰å…¨ç‚¹æ•°: {total_safe}")
    print(f"  å®‰å…¨ç‡: {overall_safety_rate*100:.1f}%")
    
    # è½¨è¿¹åˆ†ç±»ç»Ÿè®¡
    goal_trajectories = [r for r in all_results if r['goal_reached']]
    collision_trajectories = [r for r in all_results if r['collision']]
    
    print(f"\næŒ‰è½¨è¿¹ç»“æœåˆ†ç±»:")
    print(f"  åˆ°è¾¾ç›®æ ‡çš„è½¨è¿¹: {len(goal_trajectories)}")
    if goal_trajectories:
        goal_safety = np.mean([r['safety_rate'] for r in goal_trajectories])
        print(f"    å¹³å‡å®‰å…¨ç‡: {goal_safety*100:.1f}%")
    
    print(f"  ç¢°æ’çš„è½¨è¿¹: {len(collision_trajectories)}")
    if collision_trajectories:
        collision_safety = np.mean([r['safety_rate'] for r in collision_trajectories])
        print(f"    å¹³å‡å®‰å…¨ç‡: {collision_safety*100:.1f}%")
    
    # ===== âœ… ä¿®æ­£ï¼šå¢å¼ºçš„å¯è¾¾é›†å®½åº¦ç»Ÿè®¡ï¼ˆåŠ ä¸Šæœ€å°å€¼ï¼‰=====
    all_widths_v = []
    all_widths_omega = []
    
    for result in all_results:
        for r in result['results']:
            all_widths_v.append(r['width_v'])
            all_widths_omega.append(r['width_omega'])
    
    print(f"\nå¯è¾¾é›†å®½åº¦ç»Ÿè®¡:")
    print(f"  çº¿é€Ÿåº¦:")
    print(f"    æœ€å°: {np.min(all_widths_v):.6f}")  
    print(f"    å¹³å‡: {np.mean(all_widths_v):.6f}")
    print(f"    ä¸­ä½æ•°: {np.median(all_widths_v):.6f}")  # âœ… æ–°å¢
    print(f"    æ ‡å‡†å·®: {np.std(all_widths_v):.6f}")
    print(f"    æœ€å¤§: {np.max(all_widths_v):.6f}")
    print(f"    95%åˆ†ä½: {np.percentile(all_widths_v, 95):.6f}")  # âœ… æ–°å¢ï¼ˆéªŒè¯é˜ˆå€¼è®¾ç½®ï¼‰
    
    print(f"  è§’é€Ÿåº¦:")
    print(f"    æœ€å°: {np.min(all_widths_omega):.6f}")  
    print(f"    å¹³å‡: {np.mean(all_widths_omega):.6f}")
    print(f"    ä¸­ä½æ•°: {np.median(all_widths_omega):.6f}")  # âœ… æ–°å¢
    print(f"    æ ‡å‡†å·®: {np.std(all_widths_omega):.6f}")
    print(f"    æœ€å¤§: {np.max(all_widths_omega):.6f}")
    print(f"    95%åˆ†ä½: {np.percentile(all_widths_omega, 95):.6f}")  # âœ… æ–°å¢ï¼ˆéªŒè¯é˜ˆå€¼è®¾ç½®ï¼‰
    
    print(f"\næ€§èƒ½ç»Ÿè®¡:")
    print(f"  æ€»è€—æ—¶: {total_elapsed/60:.1f} åˆ†é’Ÿ ({total_elapsed/3600:.2f} å°æ—¶)")
    print(f"  å¹³å‡æ¯è½¨è¿¹: {total_elapsed/n_trajectories:.1f} ç§’")
    print(f"  å¹³å‡æ¯é‡‡æ ·ç‚¹: {total_elapsed/total_samples:.2f} ç§’")
    
    # è®¡ç®—åŠ é€Ÿæ¯”
    avg_traj_time = np.mean([r['compute_time'] for r in all_results])
    serial_time = avg_traj_time * n_trajectories
    speedup = serial_time / total_elapsed
    
    print(f"\nå¹¶è¡ŒåŠ é€Ÿ:")
    print(f"  ä¸²è¡Œé¢„è®¡è€—æ—¶: {serial_time/60:.1f} åˆ†é’Ÿ")
    print(f"  å¹¶è¡Œå®é™…è€—æ—¶: {total_elapsed/60:.1f} åˆ†é’Ÿ")
    print(f"  åŠ é€Ÿæ¯”: {speedup:.1f}x")
    print(f"  å¹¶è¡Œæ•ˆç‡: {speedup/n_workers*100:.1f}%")
    
    # ä¿å­˜ç»“æœ
    output_path = Path(__file__).parent.parent / "assets" / "reachability_results_pure_polar_lightweight_12.json"
    
    output_data = {
        'metadata': {
            'method': 'pure_polar_paper_aligned',  # âœ… ä¿®æ­£ï¼šæ›´å‡†ç¡®çš„æè¿°
            'model': 'TD3_lightweight',
            'hidden_dim': 26,
            'n_trajectories': n_trajectories,
            'total_samples': total_samples,
            'observation_error': observation_error,
            'sample_interval': sample_interval,
            'bern_order': 1,
            'error_steps': 4000,
            'n_workers': n_workers,
            'n_cores': n_cores,
            'elapsed_time': total_elapsed,
            'speedup': speedup,
            'safety_thresholds': {  # âœ… æ–°å¢ï¼šè®°å½•ä½¿ç”¨çš„é˜ˆå€¼
                # 'max_width_linear': 0.5,
                # 'max_width_angular': 0.4,
                'collision_delta': 0.4,
                'safety_margin': 0.05,
            },
            'fixes': [
                'Laser index corrected: state[0:20] instead of state[2:10]',
                'Action mapping added: (action+1)/2 for linear velocity',
                'Collision threshold aligned: 0.4m from ros_python.py',
                'Width thresholds adjusted: 0.5/0.4 (based on 95th percentile)',
                'Action range check removed: POLAR numerical expansion is normal'
            ]
        },
        'summary': {
            'overall_safety_rate': overall_safety_rate,
            'total_safe': total_safe,
            'total_samples': total_samples,
            'goal_trajectories': len(goal_trajectories),
            'collision_trajectories': len(collision_trajectories),
            # âœ… æ–°å¢ï¼šå®½åº¦ç»Ÿè®¡æ‘˜è¦
            'width_statistics': {
                'linear': {
                    'min': float(np.min(all_widths_v)),
                    'mean': float(np.mean(all_widths_v)),
                    'median': float(np.median(all_widths_v)),
                    'std': float(np.std(all_widths_v)),
                    'max': float(np.max(all_widths_v)),
                    'p95': float(np.percentile(all_widths_v, 95)),
                },
                'angular': {
                    'min': float(np.min(all_widths_omega)),
                    'mean': float(np.mean(all_widths_omega)),
                    'median': float(np.median(all_widths_omega)),
                    'std': float(np.std(all_widths_omega)),
                    'max': float(np.max(all_widths_omega)),
                    'p95': float(np.percentile(all_widths_omega, 95)),
                },
            },
        },
        'trajectories': all_results,
    }
    
    try:
        with open(output_path, 'w') as f:
            json.dump(output_data, f, indent=2)
        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        print(f"   æ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024:.1f} KB")
    except Exception as e:
        print(f"\nâŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise
    
    print("="*70)
    print("\nğŸ‰ çº¯POLARéªŒè¯å®Œæˆï¼ˆè®ºæ–‡å¯¹é½ç‰ˆï¼‰ï¼")
    print(f"ğŸ’¡ å…³é”®ä¿®æ­£:")
    print(f"   1. æ¿€å…‰æ•°æ®: state[0:20] (å®Œæ•´20ä¸ª)")
    print(f"   2. åŠ¨ä½œæ˜ å°„: (action+1)/2 for çº¿é€Ÿåº¦")
    print(f"   3. ç¢°æ’é˜ˆå€¼: 0.4m (ä¸è®­ç»ƒä¸€è‡´)")


if __name__ == "__main__":
    main()