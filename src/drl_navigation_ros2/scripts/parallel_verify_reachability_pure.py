#!/usr/bin/env python3
"""
å¹¶è¡Œå¯è¾¾æ€§éªŒè¯è„šæœ¬ - çº¯POLARç‰ˆæœ¬
ç§»é™¤å…‰çº¿æŠ•å°„ï¼Œå®Œå…¨éµå¾ªè®ºæ–‡æ–¹æ³•
"""

import sys
try:
    import distutils.version
except AttributeError:
    import distutils
    from packaging import version as packaging_version
    distutils.version = type('version', (), {
        'LooseVersion': packaging_version.Version,
        'StrictVersion': packaging_version.Version
    })
from pathlib import Path
import numpy as np
import torch
import pickle
import json
import time
from multiprocessing import Pool, cpu_count

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from TD3.TD3 import TD3


def compute_reachable_set_pure_polar(
    actor,
    state,
    observation_error=0.01,
    bern_order=1,
    error_steps=4000,
    max_action=1.0,
):
    """
    çº¯POLARå¯è¾¾æ€§éªŒè¯ - ä¸clearpath_rl_polarå®Œå…¨ä¸€è‡´
    åªè®¡ç®—å¯è¾¾é›†ï¼Œä¸åšé¢å¤–çš„ç‰©ç†éªŒè¯
    """
    import sympy as sym
    from verification.taylor_model import (
        TaylorModel,
        TaylorArithmetic,
        BernsteinPolynomial,
        compute_tm_bounds,
        apply_activation,
    )
    
    # 1. æå–Actoræƒé‡
    weights = []
    biases = []
    
    with torch.no_grad():
        for name, param in actor.named_parameters():
            if 'weight' in name:
                weights.append(param.cpu().numpy())
            elif 'bias' in name:
                biases.append(param.cpu().numpy())
    
    # 2. åˆ›å»ºç¬¦å·å˜é‡
    state_dim = len(state)
    z_symbols = [sym.Symbol(f'z{i}') for i in range(state_dim)]
    
    # 3. æ„é€ è¾“å…¥Tayloræ¨¡å‹
    TM_state = []
    for i in range(state_dim):
        poly = sym.Poly(
            observation_error * z_symbols[i] + state[i], 
            *z_symbols
        )
        TM_state.append(TaylorModel(poly, [0.0, 0.0]))
    
    # 4. é€å±‚ä¼ æ’­
    TM_input = TM_state
    TA = TaylorArithmetic()
    BP = BernsteinPolynomial(error_steps=error_steps)
    
    num_layers = len(biases)
    
    for layer_idx in range(num_layers):
        TM_temp = []
        W = weights[layer_idx]
        b = biases[layer_idx]
        
        num_neurons = len(b)
        
        for neuron_idx in range(num_neurons):
            # çº¿æ€§å˜æ¢
            tm_neuron = TA.weighted_sumforall(
                TM_input,
                W[neuron_idx],
                b[neuron_idx]
            )
            
            # æ¿€æ´»å‡½æ•°
            is_hidden = (layer_idx < num_layers - 1)
            
            if is_hidden:
                # ReLU
                a, b_bound = compute_tm_bounds(tm_neuron)
                
                if a >= 0:
                    TM_after = tm_neuron
                elif b_bound <= 0:
                    zero_poly = sym.Poly(0, *z_symbols)
                    TM_after = TaylorModel(zero_poly, [0, 0])
                else:
                    bern_poly = BP.approximate(a, b_bound, bern_order, 'relu')
                    bern_error = BP.compute_error(a, b_bound, 'relu')
                    TM_after = apply_activation(
                        tm_neuron, bern_poly, bern_error, bern_order
                    )
            else:
                # Tanh
                a, b_bound = compute_tm_bounds(tm_neuron)
                bern_poly = BP.approximate(a, b_bound, bern_order, 'tanh')
                bern_error = BP.compute_error(a, b_bound, 'tanh')
                TM_after = apply_activation(
                    tm_neuron, bern_poly, bern_error, bern_order
                )
                TM_after = TA.constant_product(TM_after, max_action)
            
            TM_temp.append(TM_after)
        
        TM_input = TM_temp
    
    # 5. è®¡ç®—åŠ¨ä½œå¯è¾¾é›†
    action_ranges = []
    for tm in TM_input:
        a, b = compute_tm_bounds(tm)
        action_ranges.append([a, b])
    
    return action_ranges


def check_action_safety_simple(action_ranges, state):
    """
    ç®€å•çš„å®‰å…¨æ€§æ£€æŸ¥ - ä¸clearpath_rl_polarä¸€è‡´
    åªåŸºäºå¯è¾¾é›†å®½åº¦å’Œæ¿€å…‰é›·è¾¾æ•°æ®
    """
    # 1. æ£€æŸ¥å¯è¾¾é›†å®½åº¦
    for i, (min_val, max_val) in enumerate(action_ranges):
        range_width = max_val - min_val
        if range_width > 1.5:
            return False
    
    # 2. æ£€æŸ¥ç¢°æ’é£é™©ï¼ˆåŸºäºæ¿€å…‰é›·è¾¾ï¼‰
    laser_readings = state[2:10]  # 8ä¸ªæ¿€å…‰æ•°æ®ï¼ˆå·²å½’ä¸€åŒ–ï¼‰
    min_laser = np.min(laser_readings)
    
    if min_laser < 0.05:  # å¾ˆè¿‘çš„éšœç¢ç‰©
        linear_vel_range = action_ranges[0]
        if linear_vel_range[1] > 0.3:  # å¯èƒ½å‰è¿›
            return False
    
    # 3. æ£€æŸ¥åŠ¨ä½œèŒƒå›´
    if action_ranges[0][0] < -0.6 or action_ranges[0][1] > 0.6:
        return False
    if action_ranges[1][0] < -1.1 or action_ranges[1][1] > 1.1:
        return False
    
    return True


def verify_single_trajectory_worker(args):
    """å•ä¸ªè½¨è¿¹çš„éªŒè¯å‡½æ•°ï¼ˆçº¯POLARç‰ˆæœ¬ï¼‰"""
    trajectory_idx, trajectory_data, model_path, observation_error, sample_interval = args
    
    # åŠ è½½æ¨¡å‹
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    agent = TD3(
        state_dim=25,
        action_dim=2,
        max_action=1.0,
        device=device,
        load_model=True,
        model_name="TD3",
        load_directory=model_path,
    )
    
    # æå–çŠ¶æ€å¹¶é‡‡æ ·
    states = trajectory_data['states']
    poses = trajectory_data['poses']
    
    sampled_states = states[::sample_interval]
    sampled_poses = poses[::sample_interval]
    n_samples = len(sampled_states)
    
    print(f"[è¿›ç¨‹ {trajectory_idx+1}] å¼€å§‹éªŒè¯ {n_samples} ä¸ªé‡‡æ ·ç‚¹ï¼ˆçº¯POLARï¼‰...")
    
    results = []
    safe_count = 0
    start_time = time.time()
    
    for i, (state, pose) in enumerate(zip(sampled_states, sampled_poses)):
        step_idx = i * sample_interval
        
        if i % max(1, n_samples // 4) == 0:
            elapsed = time.time() - start_time
            print(f"[è¿›ç¨‹ {trajectory_idx+1}] è¿›åº¦: {i+1}/{n_samples} "
                  f"({i/n_samples*100:.0f}%) | å·²ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ")
        
        # çº¯POLARè®¡ç®—å¯è¾¾é›†
        action_ranges = compute_reachable_set_pure_polar(
            agent.actor,
            state,
            observation_error=observation_error,
            bern_order=1,
            error_steps=4000,
            max_action=1.0,
        )
        
        # ç®€å•å®‰å…¨æ€§æ£€æŸ¥
        is_safe = check_action_safety_simple(action_ranges, state)
        
        det_action = agent.get_action(state, add_noise=False)
        width_v = action_ranges[0][1] - action_ranges[0][0]
        width_omega = action_ranges[1][1] - action_ranges[1][0]
        
        if is_safe:
            safe_count += 1
        
        result = {
            'step': step_idx,
            'pose': pose.tolist(),
            'det_action': det_action.tolist(),
            'action_ranges': action_ranges,
            'is_safe': is_safe,
            'width_v': float(width_v),
            'width_omega': float(width_omega),
            'min_laser': float(np.min(state[:20])),
            'distance': float(state[20]),
        }
        results.append(result)
    
    elapsed_time = time.time() - start_time
    trajectory_summary = {
        'trajectory_idx': trajectory_idx,
        'n_samples': n_samples,
        'safe_count': safe_count,
        'safety_rate': safe_count / n_samples if n_samples > 0 else 0,
        'collision': trajectory_data['collision'],
        'goal_reached': trajectory_data['goal_reached'],
        'steps': trajectory_data['steps'],
        'total_reward': float(trajectory_data['total_reward']),
        'compute_time': elapsed_time,
        'results': results,
    }
    
    print(f"[è¿›ç¨‹ {trajectory_idx+1}] âœ… å®Œæˆï¼å®‰å…¨ç‡: {trajectory_summary['safety_rate']*100:.1f}% | "
          f"è€—æ—¶: {elapsed_time/60:.1f}åˆ†é’Ÿ")
    
    return (trajectory_idx, trajectory_summary)


def load_trajectories(pkl_path=None):
    """åŠ è½½ä¿å­˜çš„è½¨è¿¹"""
    if pkl_path is None:
        pkl_path = Path(__file__).parent.parent / "assets" / "trajectories.pkl"
    
    if not pkl_path.exists():
        raise FileNotFoundError(f"è½¨è¿¹æ–‡ä»¶ä¸å­˜åœ¨: {pkl_path}")
    
    with open(pkl_path, 'rb') as f:
        trajectories = pickle.load(f)
    
    valid_trajectories = [t for t in trajectories if t is not None]
    return valid_trajectories


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print("ğŸš€ çº¯POLARå¹¶è¡ŒéªŒè¯å·¥å…·")
    print("="*70)
    
    n_cores = cpu_count()
    print(f"\næ£€æµ‹åˆ° CPU æ ¸å¿ƒæ•°: {n_cores}")
    
    print("\n[1/3] åŠ è½½è½¨è¿¹...")
    trajectories = load_trajectories()
    n_trajectories = len(trajectories)
    print(f"  âœ… åŠ è½½ {n_trajectories} æ¡è½¨è¿¹")
    
    total_states = sum(t['steps'] for t in trajectories)
    print(f"  æ€»çŠ¶æ€æ•°: {total_states}")
    
    print("\n[2/3] å‡†å¤‡å¹¶è¡Œè®¡ç®—...")
    
    model_path = project_root / "models" / "TD3" / "Nov17_06-22-08_archived"
    observation_error = 0.01
    sample_interval = 10
    
    n_workers = min(n_trajectories, n_cores // 2)
    print(f"  å¹¶è¡Œè¿›ç¨‹æ•°: {n_workers}")
    print(f"  è§‚æµ‹è¯¯å·®: Â±{observation_error}")
    print(f"  é‡‡æ ·é—´éš”: æ¯ {sample_interval} æ­¥")
    
    args_list = [
        (i, traj, model_path, observation_error, sample_interval)
        for i, traj in enumerate(trajectories)
    ]
    
    print(f"\n[3/3] å¯åŠ¨ {n_workers} ä¸ªå¹¶è¡Œè¿›ç¨‹...")
    print("="*70)
    
    start_time = time.time()
    
    with Pool(processes=n_workers) as pool:
        results = pool.map(verify_single_trajectory_worker, args_list)
    
    total_elapsed = time.time() - start_time
    
    print("\n" + "="*70)
    print("éªŒè¯ç»Ÿè®¡:")
    print("="*70)
    
    results = sorted(results, key=lambda x: x[0])
    all_results = [r[1] for r in results]
    
    total_samples = sum(r['n_samples'] for r in all_results)
    total_safe = sum(r['safe_count'] for r in all_results)
    overall_safety_rate = total_safe / total_samples if total_samples > 0 else 0
    
    print(f"\næ•´ä½“å¯è¾¾é›†å®‰å…¨æ€§:")
    print(f"  æ€»é‡‡æ ·ç‚¹: {total_samples}")
    print(f"  å®‰å…¨ç‚¹æ•°: {total_safe}")
    print(f"  å®‰å…¨ç‡: {overall_safety_rate*100:.1f}%")
    
    print(f"\næ€§èƒ½ç»Ÿè®¡:")
    print(f"  æ€»è€—æ—¶: {total_elapsed/60:.1f} åˆ†é’Ÿ")
    print(f"  å¹³å‡æ¯è½¨è¿¹: {total_elapsed/n_trajectories:.1f} ç§’")
    
    # ä¿å­˜ç»“æœ
    output_path = Path(__file__).parent.parent / "assets" / "reachability_results_pure_polar.json"
    
    output_data = {
        'metadata': {
            'method': 'pure_polar',
            'n_trajectories': n_trajectories,
            'total_samples': total_samples,
            'observation_error': observation_error,
            'sample_interval': sample_interval,
        },
        'summary': {
            'overall_safety_rate': overall_safety_rate,
            'total_safe': total_safe,
            'total_samples': total_samples,
        },
        'trajectories': all_results,
    }
    
    with open(output_path, 'w') as f:
        json.dump(output_data, f, indent=2)
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    print("="*70)
    print("\nğŸ‰ çº¯POLARéªŒè¯å®Œæˆï¼")


if __name__ == "__main__":
    main()