#!/usr/bin/env python3
"""
è½¨è¿¹æ”¶é›†è„šæœ¬
åœ¨æœ¬åœ° Gazebo ç¯å¢ƒä¸­è¿è¡Œï¼Œæ”¶é›†æ‰€æœ‰è¯„ä¼°åœºæ™¯çš„è½¨è¿¹
è¾“å‡ºï¼šä¿å­˜åˆ° assets/trajectories_lightweight.pkl
"""

import sys
from pathlib import Path
import numpy as np
import torch
import pickle
import json
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from TD3.TD3 import TD3
from TD3.TD3_lightweight import TD3 as TD3_Lightweight
from ros_python import ROS_env
from utils import pos_data


def load_eval_scenarios(json_path=None):
    """åŠ è½½è¯„ä¼°åœºæ™¯"""
    if json_path is None:
        json_path = Path(__file__).parent.parent / "assets" / "eval_scenarios.json"
    
    if not json_path.exists():
        print(f"âš ï¸  åœºæ™¯æ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
        print("   å°†ç”Ÿæˆéšæœºåœºæ™¯")
        from utils import record_eval_positions
        scenarios = record_eval_positions(
            n_eval_scenarios=10,
            save_to_file=True,
            enable_random_obstacles=False
        )
        return scenarios
    
    with open(json_path, 'r') as f:
        data = json.load(f)
    
    scenarios = []
    for scenario_dict in data['scenarios']:
        scenario = []
        for element_dict in scenario_dict['elements']:
            element = pos_data()
            element.name = element_dict['name']
            element.x = element_dict['x']
            element.y = element_dict['y']
            element.angle = element_dict['angle']
            scenario.append(element)
        scenarios.append(scenario)
    
    return scenarios


def collect_single_trajectory(agent, env, scenario, max_steps=300):
    """
    æ”¶é›†å•ä¸ªåœºæ™¯çš„è½¨è¿¹ï¼ˆæ·»åŠ ä½å§¿è¿½è¸ªï¼‰
    ä¿®å¤ï¼šä½¿ç”¨çœŸå®ä½å§¿è€Œéä¼°è®¡å€¼
    """
    from squaternion import Quaternion
    
    trajectory = []
    actions = []
    rewards = []
    poses = []  # ä¿å­˜æ¯ä¸€æ­¥çš„çœŸå®ä½å§¿
    
    # é‡ç½®ç¯å¢ƒ
    latest_scan, distance, cos, sin, collision, goal, a, reward = env.eval(scenario)
    
    # è®°å½•åˆå§‹ä¿¡æ¯
    robot_pos = (scenario[-2].x, scenario[-2].y, scenario[-2].angle)
    target_pos = (scenario[-1].x, scenario[-1].y)
    
    step_count = 0
    while step_count < max_steps:
        # ===== è·å–çœŸå®ä½å§¿ï¼ˆä»Gazeboä¼ æ„Ÿå™¨ï¼‰ =====
        latest_position = env.sensor_subscriber.latest_position
        latest_orientation = env.sensor_subscriber.latest_heading
        
        if latest_position is not None and latest_orientation is not None:
            # æå–çœŸå®çš„x, yåæ ‡
            odom_x = latest_position.x
            odom_y = latest_position.y
            # æå–çœŸå®çš„thetaè§’åº¦
            quaternion = Quaternion(
                latest_orientation.w,
                latest_orientation.x,
                latest_orientation.y,
                latest_orientation.z,
            )
            euler = quaternion.to_euler(degrees=False)
            theta = euler[2]
            current_pose = [odom_x, odom_y, theta]
        else:
            # å¦‚æœä¼ æ„Ÿå™¨æ•°æ®ä¸å¯ç”¨ï¼Œä½¿ç”¨åˆå§‹ä½å§¿
            current_pose = [robot_pos[0], robot_pos[1], robot_pos[2]]
        # ===== çœŸå®ä½å§¿è·å–ç»“æŸ =====
        
        # å‡†å¤‡çŠ¶æ€
        state, terminal = agent.prepare_state(
            latest_scan, distance, cos, sin, collision, goal, a
        )
        trajectory.append(state)
        rewards.append(reward)
        poses.append(current_pose.copy())  # ä¿å­˜çœŸå®ä½å§¿
        
        if terminal:
            break
        
        # è·å–åŠ¨ä½œ
        action = agent.get_action(state, add_noise=False)
        actions.append(action)
        a_in = [(action[0] + 1) / 2, action[1]]
        
        # æ‰§è¡ŒåŠ¨ä½œ
        latest_scan, distance, cos, sin, collision, goal, a, reward = env.step(
            lin_velocity=a_in[0], ang_velocity=a_in[1]
        )
        
        step_count += 1
    
    # æ‰“åŒ…æ•°æ®
    trajectory_data = {
        'states': np.array(trajectory),  # (T, 25)
        'actions': np.array(actions),     # (T-1, 2)
        'rewards': np.array(rewards),     # (T,)
        'poses': np.array(poses),         # â† æ–°å¢ï¼š(T, 3) - (x, y, Î¸)
        'collision': collision,
        'goal_reached': goal,
        'steps': len(trajectory),
        'total_reward': sum(rewards),
        'robot_start': robot_pos,
        'target_pos': target_pos,
    }
    
    return trajectory_data


def main():
    """ä¸»å‡½æ•°ï¼šæ”¶é›†æ‰€æœ‰åœºæ™¯çš„è½¨è¿¹"""
    print("\n" + "="*70)
    print("è½¨è¿¹æ”¶é›†å·¥å…·")
    print("="*70)
    
    # ===== 1. åŠ è½½æ¨¡å‹ =====
    print("\n[1/4] åŠ è½½æ¨¡å‹...")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åŠ è½½ best ç‰ˆæœ¬çš„æ¨¡å‹
    agent = TD3_Lightweight(
        state_dim=25,
        action_dim=2,
        max_action=1.0,
        device=device,
        load_model=True,
        model_name="TD3_lightweight_best",  # â† åŠ è½½ best ç‰ˆæœ¬
        load_directory=Path("models/TD3_lightweight/Nov24_22-43-08_cheeson"),
    )
    print(f"  âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # ===== 2. åˆå§‹åŒ–ç¯å¢ƒ =====
    print("\n[2/4] åˆå§‹åŒ– ROS ç¯å¢ƒ...")
    env = ROS_env(enable_random_obstacles=False)
    print("  âœ… ROS ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ")
    
    # ===== 3. åŠ è½½åœºæ™¯ =====
    print("\n[3/4] åŠ è½½è¯„ä¼°åœºæ™¯...")
    scenarios = load_eval_scenarios()
    n_scenarios = len(scenarios)
    print(f"  âœ… åŠ è½½ {n_scenarios} ä¸ªåœºæ™¯")
    
    # ===== 4. æ”¶é›†æ‰€æœ‰è½¨è¿¹ =====
    print(f"\n[4/4] æ”¶é›† {n_scenarios} ä¸ªåœºæ™¯çš„è½¨è¿¹...")
    print("  (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...)\n")
    
    all_trajectories = []
    
    for i, scenario in enumerate(tqdm(scenarios, desc="æ”¶é›†è½¨è¿¹")):
        try:
            trajectory_data = collect_single_trajectory(
                agent, env, scenario, max_steps=300
            )
            all_trajectories.append(trajectory_data)
            
            # æ‰“å°ç®€è¦ä¿¡æ¯
            status = "ğŸ¯ ç›®æ ‡" if trajectory_data['goal_reached'] else "ğŸ’¥ ç¢°æ’"
            print(f"  åœºæ™¯ {i+1}/{n_scenarios}: {status} | "
                  f"æ­¥æ•°={trajectory_data['steps']} | "
                  f"å¥–åŠ±={trajectory_data['total_reward']:.1f}")
        
        except Exception as e:
            print(f"  âŒ åœºæ™¯ {i+1} å¤±è´¥: {e}")
            all_trajectories.append(None)
    
    # ===== 5. ä¿å­˜è½¨è¿¹ =====
    output_path = Path(__file__).parent.parent / "assets" / "trajectories_lightweight.pkl"
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'wb') as f:
        pickle.dump(all_trajectories, f)
    
    print(f"\nâœ… æ‰€æœ‰è½¨è¿¹å·²ä¿å­˜åˆ°: {output_path}")
    
    # ===== 6. ç»Ÿè®¡ä¿¡æ¯ =====
    print("\n" + "="*70)
    print("æ”¶é›†ç»Ÿè®¡:")
    
    successful = [t for t in all_trajectories if t is not None]
    print(f"  æˆåŠŸæ”¶é›†: {len(successful)}/{n_scenarios}")
    
    goal_count = sum(1 for t in successful if t['goal_reached'])
    collision_count = sum(1 for t in successful if t['collision'])
    
    print(f"  åˆ°è¾¾ç›®æ ‡: {goal_count} ({goal_count/len(successful)*100:.1f}%)")
    print(f"  å‘ç”Ÿç¢°æ’: {collision_count} ({collision_count/len(successful)*100:.1f}%)")
    
    total_steps = sum(t['steps'] for t in successful)
    avg_steps = total_steps / len(successful)
    print(f"  æ€»æ­¥æ•°: {total_steps}")
    print(f"  å¹³å‡æ­¥æ•°: {avg_steps:.1f}")
    
    avg_reward = np.mean([t['total_reward'] for t in successful])
    print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
    
    print("="*70)
    print("\nğŸ‰ è½¨è¿¹æ”¶é›†å®Œæˆï¼")
    print(f"ç°åœ¨å¯ä»¥å°† {output_path} å¤åˆ¶åˆ°æœåŠ¡å™¨è¿›è¡Œæ‰¹é‡éªŒè¯")


if __name__ == "__main__":
    main()